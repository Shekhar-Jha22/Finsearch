# TRAINING THE MODEL, TESTING AND TUNING, COMPARING

Train the RL agent using historical data and an RL algorithm
Evaluate the agentâ€™s performance using a separate testing
dataset
Fine-tune the model and parameters to improve performance
Exercise caution when applying RL agents to real-time trading
scenarios.
Create RL and the benchmark models
(ARIMA or LSTM based) and compare the
performance
Your comparison must include both returns and risk. It may be
helpful to think about different ways to segment the data into
train and test. (the following paper may help in creating the RL
environment and variable selection:

___

# Resources

- ðŸ“š [Research paper for RL Environment](https://dl.acm.org/doi/abs/10.1145/3383455.3422540)
- ðŸ“š [Research paper for variable selection](https://neptune.ai/blog/arima-vs-prophet-vs-lstm)
